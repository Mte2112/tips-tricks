{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf9fd773-aa68-4ca9-a20b-7026e89c34e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Computing area weighted average for global/zonal means (& monthly anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5368f2-6a3d-449d-99df-4d42b9d6c6db",
   "metadata": {},
   "source": [
    "#### Functions for area weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eb41211-be39-4115-afd5-431f26b95e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from xarray import DataArray\n",
    "from numpy import meshgrid, deg2rad, gradient, cos\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514063a-a86e-4c60-8264-a303b88b3beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import file(s)\n",
    "ds = xr.load_dataset('/path/to/dataset_file.nc')\n",
    "df = pd.read_csv('/path/to/file.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11f53a1e-2719-491c-abb1-f09eac2e687d",
   "metadata": {
    "id": "igm-DOcdCXEk"
   },
   "outputs": [],
   "source": [
    "# Calculation for grid cell area\n",
    "def area_grid(lat, lon):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate the area of each grid cell\n",
    "    (lat, lon)--> (lat, lon), grid-cell area (m**2)\n",
    "    \"\"\"\n",
    "\n",
    "    x, y = meshgrid(lon, lat)\n",
    "    R = radius(y)\n",
    "    rlon = deg2rad(gradient(x, axis=1))\n",
    "    rlat = deg2rad(gradient(y, axis=0))\n",
    "    rx = R*rlon*cos(deg2rad(y))\n",
    "    ry = R*rlat\n",
    "  \n",
    "    area = ry*rx\n",
    "\n",
    "    data_array = DataArray(\n",
    "        area, dims=[\"lat\", \"lon\"], \n",
    "        coords={\"lat\": lat, \"lon\": lon})\n",
    "    return data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58a14013-e80a-43ea-a76e-de70c73d3baa",
   "metadata": {
    "id": "auB_lDQ-CXHH"
   },
   "outputs": [],
   "source": [
    "# Note: this script **does not** make the assumption that Earth is a perfect sphere. It is more accurately defined as a spheroid below\n",
    "def radius(lat):\n",
    "    \n",
    "    '''\n",
    "    calculate radius of Earth using lat (degrees)\n",
    "    lat--> radius(m)\n",
    "    '''\n",
    "    \n",
    "    # Since Earth isn't a perfect sphere, must define the spheroid (sphere-like object)\n",
    "    a = 6378137.0 #equatorial radius (m)\n",
    "    b = 6356752.3142 #radius at poles (m)\n",
    "    e2 = 1 - (b**2/a**2)\n",
    "    \n",
    "    # convert to geocentric\n",
    "    lat = deg2rad(lat)\n",
    "    lat_gc = np.arctan( (1-e2)*np.tan(lat) )\n",
    "\n",
    "    # calculate radius\n",
    "    r = ((a * (1 - e2)**0.5)/(1 - (e2 * np.cos(lat_gc)**2))**0.5)\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2546fd99-7af6-49b5-881e-59eba8075b1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Addressing gaps in the data (if any) \n",
    "It is very important to identify if there are gaps in the data before proceeding. If this is a complete dataset, then proceed. If the gaps are somewhat insignificant, perhaps try \"nearest neighbor\" interpolation or another technique. For larger data gaps, a more sophisticated statistical technique for weighting may be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfdb2d2-3d46-4364-b6c6-a028b9c17753",
   "metadata": {},
   "source": [
    "*Simple interpolation example using np.interp*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1c63e50-40db-424a-81b8-586c9de6f9cc",
   "metadata": {
    "id": "piwvUK6ZLsAw"
   },
   "outputs": [],
   "source": [
    "# Convert gaps to NaN (if they aren't NaN already) and interpolate these values in the next step. \n",
    "# Below is an example approach using a pandas dataframe. If working with a dataset, the approach will not be much different\n",
    "## Note: gaps may be stored as -9999 or another number. A quick way to check is to plot a time series of the data and visually see the extremes that don't fit\n",
    "## Another note: if area-weighting a variable other than temp, replace 'temp' with variable name\n",
    "df['temp'] = df['temp'].replace(9999, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59438baf-b424-41ea-addb-5b68898b48fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qk0KN0c03bV2",
    "outputId": "3048ec4c-4498-4175-defd-1cceb2e43999"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Interpolate NaN temp values\n",
    "temp_nan = df['temp']\n",
    "mask = np.isnan(temp_nan)\n",
    "temp_nan[mask] = np.interp(np.flatnonzero(mask), \n",
    "                       np.flatnonzero(~mask), \n",
    "                       temp_nan[~mask])\n",
    "\n",
    "df['temp'] = temp_nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ba73f1-fbba-4cc7-a9a5-4eb1fbbf6ed5",
   "metadata": {},
   "source": [
    "#### Compute weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f8a9f304-f989-4d6f-b910-10ebf1765581",
   "metadata": {
    "id": "0adKgcNpCXKE"
   },
   "outputs": [],
   "source": [
    "# If working with a dataframe, convert the df to an xarray dataset first. Or manipulate the code below to be compatible with the dataframe.\n",
    "\n",
    "# Example of df to ds conversion \n",
    "# Step 1: set index of dataframe\n",
    "df = df.set_index(['lon', 'lat', 'time'])\n",
    "# Step 2: convert to xarray dataset\n",
    "ds = df.to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95298726-7b52-4bf9-971c-82df3c62f124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute weighting where 'ds' is the name of the dataset. replace ds with your dataset name. \n",
    "# Area dataArray\n",
    "array_area = area_grid(ds['lat'], ds['lon'])\n",
    "# Total area\n",
    "total_area = array_area.sum(['lat','lon'])\n",
    "# Area-weighted temp\n",
    "temp_weighted = (ds['temp']*array_area)/total_area\n",
    "# weighting\n",
    "weight_tot = (array_area/total_area).sum(('lat','lon'))\n",
    "# global average. If you are only interested in the global mean temp, the calculation is simple. Just sum all weighted values to get the average.\n",
    "global_mean = temp_weighted.sum(dim=('lat','lon'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4205df-19ad-4f44-957b-7e91efb90dfc",
   "metadata": {},
   "source": [
    "#### Define zones and compute averages for anomaly calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "857801fc-3a77-4661-98eb-ebea245c04b3",
   "metadata": {
    "id": "xDW6EgbahxEH"
   },
   "outputs": [],
   "source": [
    "# define area array for cells and sum for each zone\n",
    "# this will compute the weighting for each zone. this will be used to get the nominal zonal averages \n",
    "# variable interpretation: ex. N24N90 = from 24째N to 90째N, S24N24 = from 24째S to 24째N \n",
    "\n",
    "NHem = (array_area.where(array_area['lat'] >= 0)/total_area).sum(('lat','lon'))\n",
    "SHem = (array_area.where(array_area['lat'] <= 0)/total_area).sum(('lat','lon'))\n",
    "N24N90 = (array_area.where((array_area['lat'] >= 24) & (array_area['lat'] <= 90))/total_area).sum(('lat','lon'))\n",
    "S24N24 = (array_area.where((array_area['lat'] >= -24) & (array_area['lat'] <= 24))/total_area).sum(('lat','lon'))\n",
    "S24S90 = (array_area.where((array_area['lat'] >= -90) & (array_area['lat'] <= -24))/total_area).sum(('lat','lon'))\n",
    "N64N90 = (array_area.where((array_area['lat'] >= 64) & (array_area['lat'] <= 90))/total_area).sum(('lat','lon'))\n",
    "N44N64 = (array_area.where((array_area['lat'] >= 44) & (array_area['lat'] <= 64))/total_area).sum(('lat','lon'))\n",
    "N24N44 = (array_area.where((array_area['lat'] >= 24) & (array_area['lat'] <= 44))/total_area).sum(('lat','lon'))\n",
    "EQUN24 = (array_area.where((array_area['lat'] >= 0) & (array_area['lat'] <= 24))/total_area).sum(('lat','lon'))\n",
    "EQUS24 = (array_area.where((array_area['lat'] >= -24) & (array_area['lat'] <= 0))/total_area).sum(('lat','lon'))\n",
    "S24S44 = (array_area.where((array_area['lat'] >= -44) & (array_area['lat'] <= -24))/total_area).sum(('lat','lon'))\n",
    "S44S64 = (array_area.where((array_area['lat'] >= -64) & (array_area['lat'] <= -44))/total_area).sum(('lat','lon'))\n",
    "S64S90 = (array_area.where((array_area['lat'] >= -90) & (array_area['lat'] <= -64))/total_area).sum(('lat','lon'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "23e045d8-0fa6-42ab-b469-c5a48e483d76",
   "metadata": {
    "id": "WyPFCJ4nCXMr"
   },
   "outputs": [],
   "source": [
    "# Mask by zone. Zones are broken up as follows: {tropics:(0-24), subtropics:(24-44), extratropics=(44-64), polar:(64-90)}\n",
    "# NOTE: zonal values summed will = global mean. Zonal values as is need to be divided by the respective zonal weighting to get nominal zonal average\n",
    "\n",
    "n_polar = temp_weighted.where((temp_weighted['lat'] >= 64.0) & (temp_weighted['lat'] <= 90.0), \n",
    "                             drop=True).sum(dim=('lat', 'lon'))\n",
    "n_extratropics = temp_weighted.where((temp_weighted['lat'] >= 44.0) & (temp_weighted['lat'] <= 64.0), \n",
    "                                 drop=True).sum(dim=('lat', 'lon'))\n",
    "n_subtropics = temp_weighted.where((temp_weighted['lat'] >= 24) & (temp_weighted['lat'] <= 44.0), \n",
    "                                   drop=True).sum(dim=('lat', 'lon'))\n",
    "n_tropics = temp_weighted.where((temp_weighted['lat'] >= 0.0) & (temp_weighted['lat'] <= 24), \n",
    "                                  drop=True).sum(dim=('lat', 'lon'))\n",
    "s_tropics = temp_weighted.where((temp_weighted['lat'] >= -24) & (temp_weighted['lat'] <= 0.0), \n",
    "                                  drop=True).sum(dim=('lat', 'lon'))\n",
    "s_subtropics = temp_weighted.where((temp_weighted['lat'] >= -44) & (temp_weighted['lat'] <= -24), \n",
    "                                 drop=True).sum(dim=('lat', 'lon'))\n",
    "s_extratropics = temp_weighted.where((temp_weighted['lat'] >= -64.0) & (temp_weighted['lat'] <= -44.0), \n",
    "                                   drop=True).sum(dim=('lat', 'lon'))\n",
    "s_polar = temp_weighted.where((temp_weighted['lat'] >= -90.0) & (temp_weighted['lat'] <= -64.0), \n",
    "                                   drop=True).sum(dim=('lat', 'lon'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "43f8767f-4853-4fef-83e7-d1a0f57b603d",
   "metadata": {
    "id": "BxaKHnXjlj5-"
   },
   "outputs": [],
   "source": [
    "# To compute monthly anomalies for a given period, calculate the mean for the base period. \n",
    "# Note: using a base period of 2007-2016. Change base period if necessary\n",
    "base = temp_weighted.sel(time=slice(\"2007-01-01\", \"2016-12-31\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "8fa4ff5b-bacc-4429-9434-4d331832f9cb",
   "metadata": {
    "id": "riMhRWi9oBNv"
   },
   "outputs": [],
   "source": [
    "# Calculate base mean temperature values for zone and time, **weighted values, not nominal** \n",
    "n_polar_base = base.where((base['lat'] >= 64.0) & (base['lat'] <= 90.0), \n",
    "                             drop=True).sum(dim=('lat', 'lon'))\n",
    "n_extratropics_base = base.where((base['lat'] >= 44.0) & (base['lat'] <= 64.0), \n",
    "                                 drop=True).sum(dim=('lat', 'lon'))\n",
    "n_subtropics_base = base.where((base['lat'] >= 24) & (base['lat'] <= 44.0), \n",
    "                                   drop=True).sum(dim=('lat', 'lon'))\n",
    "n_tropics_base = base.where((base['lat'] >= 0.0) & (base['lat'] <= 24), \n",
    "                                  drop=True).sum(dim=('lat', 'lon'))\n",
    "s_tropics_base = base.where((base['lat'] >= -24) & (base['lat'] <= 0.0), \n",
    "                                  drop=True).sum(dim=('lat', 'lon'))\n",
    "s_subtropics_base = base.where((base['lat'] >= -44) & (base['lat'] <= -24), \n",
    "                                 drop=True).sum(dim=('lat', 'lon'))\n",
    "s_extratropics_base = base.where((base['lat'] >= -64.0) & (base['lat'] <= -44.0), \n",
    "                                   drop=True).sum(dim=('lat', 'lon'))\n",
    "s_polar_base = base.where((base['lat'] >= -90.0) & (base['lat'] <= -64.0), \n",
    "                                   drop=True).sum(dim=('lat', 'lon'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb745c86-ed14-4767-8ca8-3fadb187cd77",
   "metadata": {},
   "source": [
    "#### Compute anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d4f344ec-f706-4102-a167-0f3c171369d7",
   "metadata": {
    "id": "DxuMMJWv_3Z0"
   },
   "outputs": [],
   "source": [
    "# Compute climatological mean for the base period\n",
    "# Subtract that value from the monthly mean weighted temp for each month in each zone to get monthly anom\n",
    "# Note: using base period and not using any kind of running mean. If you want to compute the anomaly using a running mean and need a script, reach out to me @ maxwell.t.elling@nasa.gov\n",
    "\n",
    "n_polar_anom = n_polar.groupby('time.month') - n_polar_base.groupby('time.month').mean(dim=('time'))\n",
    "n_extratropics_anom = n_extratropics.groupby('time.month') - n_extratropics_base.groupby('time.month').mean(dim=('time'))\n",
    "n_subtropics_anom = n_subtropics.groupby('time.month') - n_subtropics_base.groupby('time.month').mean(dim=('time'))\n",
    "n_tropics_anom = n_tropics.groupby('time.month') - n_tropics_base.groupby('time.month').mean(dim=('time'))\n",
    "s_tropics_anom = s_tropics.groupby('time.month') - s_tropics_base.groupby('time.month').mean(dim=('time'))\n",
    "s_subtropics_anom = s_subtropics.groupby('time.month') - s_subtropics_base.groupby('time.month').mean(dim=('time'))\n",
    "s_extratropics_anom = s_extratropics.groupby('time.month') - s_extratropics_base.groupby('time.month').mean(dim=('time'))\n",
    "s_polar_anom = s_polar.groupby('time.month') - s_polar_base.groupby('time.month').mean(dim=('time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "83bb42cd-245c-4666-992f-dd956cb4470a",
   "metadata": {
    "id": "cxmZy5qZFCX7"
   },
   "outputs": [],
   "source": [
    "# Sum the monthly zonal anoms to get the global mean anom. Note: results are in 째C\n",
    "\n",
    "''' Weighting was calculated via dividing weighting area by total global area, \n",
    "resulting in temp values that are weighted unproportionately to the zonal area. \n",
    "The quick fix is to divide by zonal area.\n",
    "\n",
    "Zonal and hemispheric means must be divided by the repsective total zonal weighting \n",
    "because the zonal anomalies above were weighted using the total area over the whole globe.\n",
    "\n",
    "When you divide by the total zonal weighting, you are essentially weighting the grid cells \n",
    "of each zone against the total area of that zone, which will produce correct values. \n",
    "\n",
    "If you do not divide by the respective zonal weight, the sum of grid cell weights in each zone \n",
    "will not add up to 100% (rather the weights for all the zones will add to 100%). This will \n",
    "cause the temp values to be much smaller (and incorrect).\n",
    "'''\n",
    "\n",
    "global_mean_anomaly = n_polar_anom+n_extratropics_anom+n_subtropics_anom+n_tropics_anom+s_tropics_anom+s_subtropics_anom+s_extratropics_anom+s_polar_anom\n",
    "\n",
    "NHem = (n_polar_anom+n_extratropics_anom+n_subtropics_anom+n_tropics_anom)/(NHem)\n",
    "SHem = (s_tropics_anom+s_subtropics_anom+s_extratropics_anom+s_polar_anom)/(SHem)\n",
    "N24N90 = (n_polar_anom+n_extratropics_anom+n_subtropics_anom)/(N24N90)\n",
    "S24N24 = (n_tropics_anom+s_tropics_anom)/(S24N24)\n",
    "S24S90 = (s_subtropics_anom+s_extratropics_anom+s_polar_anom)/(S24S90)\n",
    "N64N90 = (n_polar_anom)/(N64N90)\n",
    "N44N64 = (n_extratropics_anom)/(N44N64)\n",
    "N24N44 = (n_subtropics_anom)/(N24N44)\n",
    "EQUN24 = (n_tropics_anom)/(EQUN24)\n",
    "EQUS24 = (s_tropics_anom)/(EQUS24)\n",
    "S24S44 = (s_subtropics_anom)/(S24S44)\n",
    "S44S64 = (s_extratropics_anom)/(S44S64)\n",
    "S64S90 = (s_polar_anom)/(S64S90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
